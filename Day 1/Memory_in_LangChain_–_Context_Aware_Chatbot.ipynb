{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Practical: Memory in LangChain – Context-Aware Chatbot\n",
        "#Objective\n",
        "\n",
        "> Understand different memory types in LangChain\n",
        "\n",
        "> Implement Conversation Buffer Memory\n",
        "\n",
        ">Implement Conversation Summary Memory\n",
        "\n",
        ">Implement Vector Store–based Memory\n",
        "\n",
        ">Build a chatbot and verify memory persistence\n",
        "\n",
        "#Step 1: Install Required Packages"
      ],
      "metadata": {
        "id": "6NBtq1LvzSh_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqb1ZID_wpMA"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain==0.2.* langchain-core==0.2.* langchain-community==0.2.* langchain-google-genai faiss-cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Configure Gemini Flash LLM"
      ],
      "metadata": {
        "id": "xMt_OinfzduO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyD67leHQN0NV5z7WTnV1b_6iduUrIPzB2o\"\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-flash-latest\",\n",
        "    temperature=0.2\n",
        ")\n"
      ],
      "metadata": {
        "id": "LtwdtoA_wsbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Conversation Buffer Memory\n",
        "\n",
        "Concept: Stores the entire conversation history verbatim.\n",
        "\n",
        "Implementation"
      ],
      "metadata": {
        "id": "4pF4OOyuzgJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "buffer_memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "buffer_chatbot = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=buffer_memory,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "dOQYIZ6twzhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Buffer Memory"
      ],
      "metadata": {
        "id": "OsjrsdGFziXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_chatbot.predict(input=\"My name is Rahul and I work in compliance.\")\n",
        "buffer_chatbot.predict(input=\"What is my profession?\")\n"
      ],
      "metadata": {
        "id": "_5HEJcWvw0y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ The chatbot remembers exact past messages.\n",
        "\n",
        "#Step 4: Conversation Summary Memory\n",
        "\n",
        "Concept: Summarizes older conversations to save tokens.\n",
        "\n",
        "Implementation"
      ],
      "metadata": {
        "id": "RMDANRefznjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "summary_memory = ConversationSummaryMemory(\n",
        "    llm=llm,\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "summary_chatbot = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=summary_memory,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "931hggl3w11L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Summary Memory"
      ],
      "metadata": {
        "id": "npluWtl1zqhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_chatbot.predict(input=\"I am a risk analyst in a banking organization.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "N3JhQDRHw2u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Older context is compressed into a summary, not lost.\n",
        "\n",
        "#Step 5: Vector Store Memory (Long-Term Memory)\n",
        "\n",
        "Concept: Stores past interactions as embeddings for semantic recall.\n",
        "\n",
        "Create Vector Store Memory"
      ],
      "metadata": {
        "id": "OxKqB_9TztEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-002\"\n",
        ")\n",
        "\n",
        "# Initialize with some example texts to avoid IndexError\n",
        "example_texts = [\n",
        "    \"I am a risk analyst in a banking organization.\"\n",
        "\n",
        "]\n",
        "\n",
        "vectorstore = FAISS.from_texts(\n",
        "    texts=example_texts,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "vector_memory = VectorStoreRetrieverMemory(\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        ")"
      ],
      "metadata": {
        "id": "UkyOJKjDw5ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Chatbot with Vector Memory"
      ],
      "metadata": {
        "id": "yDXkaYLhzxaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"],\n",
        "    template=\"\"\"\n",
        "Relevant past information:\n",
        "{history}\n",
        "\n",
        "User question:\n",
        "{input}\n",
        "\n",
        "Answer clearly using past context if relevant.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "vector_chatbot = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=vector_memory,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "b1jybX8iw-SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Vector Memory (Semantic Recall)"
      ],
      "metadata": {
        "id": "SJVyy67-zzny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_chatbot.predict(input=\"I specialize in AML and KYC compliance.\")\n",
        "vector_chatbot.predict(input=\"I work closely with audit teams.\")\n",
        "vector_chatbot.predict(input=\"What compliance areas do I specialize in?\")\n"
      ],
      "metadata": {
        "id": "-EGqXiUvw_aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ The chatbot retrieves semantically relevant past context, not just recent messages.\n",
        "\n",
        "#Step 6: Memory Persistence Test"
      ],
      "metadata": {
        "id": "gNLOfM0bz11H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Buffer Memory:\")\n",
        "print(buffer_memory.buffer)\n",
        "\n",
        "print(\"\\nSummary Memory:\")\n",
        "print(summary_memory.buffer)\n"
      ],
      "metadata": {
        "id": "_ngh3mtdxBVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This confirms context retention after multiple interactions.\n",
        "\n",
        "#Memory Types Comparison\n",
        "\n",
        "| Memory Type                | Use Case                         |\n",
        "| -------------------------- | -------------------------------- |\n",
        "| ConversationBufferMemory   | Short conversations, debugging   |\n",
        "| ConversationSummaryMemory  | Long chats with token efficiency |\n",
        "| VectorStoreRetrieverMemory | Long-term, semantic memory       |\n"
      ],
      "metadata": {
        "id": "ffbZUa68z6WF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "od8AfHeSxCqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}